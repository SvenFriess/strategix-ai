<h1>Lokale LLMs: Einführung & Setup</h1>
<p>Kurzer Überblick über lokale Modelle (Mistral, Gemma, Qwen) und Ollama-Installation.</p>
<ol>
  <li>Installiere Ollama und starte den Server: <code>ollama serve</code></li>
  <li>Modelle laden: <code>ollama pull mistral</code> (oder gemma/qwen)</li>
  <li>Erster Test: <code>ollama run mistral</code></li>
</ol>